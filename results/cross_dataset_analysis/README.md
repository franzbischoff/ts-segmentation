# Cross-Dataset Analysis: Three Options (Session 10 - 2025-12-14)

**Objetivo**: Avaliar robustez de detectores combinando m√∫ltiplas dimens√µes:
1. **Op√ß√£o 1** - Ceiling performance (m√°ximo ating√≠vel com tuning local)
2. **Op√ß√£o 2** - Parameter portability (capacidade de transfer√™ncia entre datasets)
3. **Op√ß√£o 3** - Unified robustness score (combina√ß√£o de ambas dimens√µes)

---

## üìä Resumo Executivo: Tr√™s Perspectivas

| Op√ß√£o | Foco | Melhor Detector | M√©trica | Use Case |
|-------|------|------------------|---------|----------|
| **Op√ß√£o 1** | üéØ Performance m√°xima (ceiling) | **FLOSS** | F3=0.4285 | Research, max performance |
| **Op√ß√£o 2** | üöÄ Portabilidade (transfer) | **ADWIN** | 94.90% retention | Production ready |
| **Op√ß√£o 3** | ‚öñÔ∏è Robustez unificada | **FLOSS** | Score=0.9763 | Holistic selection |

### Trade-off Principal

```
FLOSS:    0.4285 ceiling ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                    ‚îÇ vs
ADWIN:    0.2879 ceiling ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí 94.90% transfer

"FLOSS Paradox": melhor performance quando retuned, mas pior portabilidade
```

---

## üìÅ Ficheiros Gerados

### Visualiza√ß√£o √önica (Op√ß√µes 1, 2 e 3)
- [results/cross_dataset_analysis/option123_summary.png](results/cross_dataset_analysis/option123_summary.png)
- Como gerar: `python -m src.visualize_option123`
- Eixo X: ceiling F3 (Op√ß√£o 1); Eixo Y: transferability m√©dia (Op√ß√£o 2); cor/tamanho: score unificado (Op√ß√£o 3)

### Op√ß√£o 1: Cross-Dataset Generalization
- **CSV**: `cross_dataset_generalization_option1.csv`
  - Columns: detector, mean_cross_fold_f3, median, std_dev, min, max, cv%, avg_gap
  - Agrega√ß√£o: Macro-average de F3 cross-fold de todos os datasets
- **Markdown**: `cross_dataset_generalization_option1.md`
  - Tabela de ranking (6 detectores)
  - Analysis por detector com F3 ceiling e intra-dataset consistency
  - Production guidance

### Op√ß√£o 2: Parameter Portability
- **CSV**: `parameter_portability_option2.csv`
  - Columns: detector, source_dataset, target_dataset, source_cross_f3, target_transferred_f3, target_local_best_f3, transferability_ratio, performance_drop, performance_drop_pct, interpretation
  - 34 transfers testadas (6 detectores √ó 3 sources √ó 2 targets)
- **Markdown**: `parameter_portability_option2.md`
  - Trade-off table (Option 1 vs Option 2)
  - Critical insights (FLOSS paradox, KSWIN sweet spot, ADWIN robustness)
  - 4 production scenarios com F3 esperado e tempo de setup
  - Decision matrix por use case

### Op√ß√£o 3: Unified Robustness Score
- **CSV**: `unified_robustness_option3.csv`
  - Columns: rank, detector, unified_score, intra_consistency, inter_generalization, ceiling_f3, avg_transferability, cv_transferability
  - F√≥rmula: `0.6 √ó (1 - 2fold_gap) + 0.4 √ó (1 - transfer_variance)`
- **Markdown**: `unified_robustness_option3.md`
  - Final ranking com scores (0 to 1 scale)
  - Top 3 recommendations com breakdown
  - Interpretation guide (o que cada componente mede)
  - Production guidance by score range
  - Comparison of all 3 options

---

## üéØ Final Ranking por Op√ß√£o

### Op√ß√£o 1: Ceiling Performance (com tuning)
```
1. FLOSS       F3 = 0.4285  (50% better than average)
2. KSWIN       F3 = 0.3176
3. Page-Hinkley F3 = 0.3132
4. HDDM_A      F3 = 0.2997
5. ADWIN       F3 = 0.2879
6. HDDM_W      F3 = 0.1527
```

### Op√ß√£o 2: Parameter Portability (transfer retention)
```
1. ADWIN        94.90% (excellent, zero poor transfers)
2. KSWIN        87.84% (stable, balanced)
3. FLOSS        75.85% (-24% loss vs ceiling!)
4. HDDM_A       65.17%
5. Page-Hinkley 54.31%
6. HDDM_W       45.64%
```

### Op√ß√£o 3: Unified Robustness Score
```
1. FLOSS        0.9763  (best intra + good inter)
2. ADWIN        0.9713  (excellent inter, poor intra)
3. KSWIN        0.9690  (balanced both)
4. HDDM_A       0.9509
5. HDDM_W       0.9426
6. Page-Hinkley 0.9049
```

---

## üéØ Recomenda√ß√µes por Cen√°rio

### Cen√°rio 1: Research / Benchmarking
- **Detector**: FLOSS
- **Setup**: Grid search com Parameters otimizados localmente
- **Performance**: F3 ‚âà 0.42 (ceiling)
- **Tempo**: Horas (tuning obrigat√≥rio)
- **Refer√™ncia**: Op√ß√£o 1

### Cen√°rio 2: Production Immediato (sem labels)
- **Detector**: ADWIN
- **Setup**: Use par√¢metros pr√©-tuned (afib_paroxysmal)
- **Performance**: F3 ‚âà 0.27 (95% de retention)
- **Tempo**: Minutos (sem tuning)
- **Refer√™ncia**: Op√ß√£o 2

### Cen√°rio 3: Production com Valida√ß√£o
- **Detector**: KSWIN
- **Setup**: Par√¢metros de afib_paroxysmal + quick validation
- **Performance**: F3 ‚âà 0.28 (88% de retention)
- **Tempo**: Minutos + valida√ß√£o r√°pida
- **Refer√™ncia**: Op√ß√£o 3 (melhor trade-off)

### Cen√°rio 4: Dados Heterog√™neos
- **Detector**: Ensemble (ADWIN + KSWIN)
- **Setup**: Voting/weighted ensemble
- **Performance**: F3 ‚âà 0.30+ (compensate individual weaknesses)
- **Tempo**: Setup ensemble + tuning
- **Refer√™ncia**: Op√ß√µes 2 + 3

---

## üîç Como Interpretar Cada M√©trica

### Op√ß√£o 1: Mean Cross-Fold F3
- **O que mede**: Melhor performance poss√≠vel ap√≥s tuning local
- **Alto (>0.35)**: Detector pode ser muito bom se bem tuned
- **Baixo (<0.25)**: Detector tem limita√ß√µes fundamentais
- **Uso**: Research, max performance goals

### Op√ß√£o 2: Transferability Ratio
- **O que mede**: % de performance retained ao transferir params
- **Excellent (>0.95)**: Params completamente port√°veis
- **Good (0.85-0.95)**: Alguns ajustes, mas baseline s√≥lido
- **Moderate (0.75-0.84)**: Tuning recomendado
- **Poor (<0.60)**: Avoid sem re-tuning
- **Uso**: Production, quick deployment

### Op√ß√£o 3: Unified Robustness Score
- **O que mede**: Combina√ß√£o balanceada de consist√™ncia + portabilidade
- **Formula**: 60% intra-dataset + 40% inter-dataset
- **Excellent (0.85-1.0)**: Production-ready
- **Good (0.75-0.84)**: Production-viable com validation
- **Acceptable (0.60-0.74)**: Poss√≠vel com monitoring
- **Poor (<0.60)**: Research only
- **Uso**: Holistic detector selection

---

## üìö Scripts Execut√°veis

### Regenerar Op√ß√£o 1
```bash
python -m src.aggregate_twofold_analysis
```
Output: `cross_dataset_generalization_option1.{csv,md}`

### Regenerar Op√ß√£o 2
```bash
python -m src.test_parameter_portability
```
Output: `parameter_portability_option2.{csv,md}`

### Regenerar Op√ß√£o 3
```bash
python -m src.unified_robustness_score
```
Output: `unified_robustness_option3.{csv,md}`

---

## üß† Key Insights

### Insight 1: FLOSS Paradox
- **Ceiling** (Op√ß√£o 1): F3=0.4285 (ü•á melhor)
- **Portability** (Op√ß√£o 2): 75.85% (-24% loss vs ceiling)
- **Implica√ß√£o**: FLOSS nunca deve ser usado em production sem re-tuning
- **Recomenda√ß√£o**: Research-only, ou produ√ß√£o com validation custosa

### Insight 2: KSWIN = Sweet Spot
- **Ceiling**: F3=0.3176 (ü•à 2¬∫ melhor)
- **Portability**: 87.84% (ü•à 2¬∫ melhor)
- **CV**: 34% (stable across datasets)
- **Implica√ß√£o**: Best balance para ambas dimens√µes
- **Recomenda√ß√£o**: Primeira escolha para production com constraints

### Insight 3: ADWIN Robustness
- **Ceiling**: F3=0.2879 (5¬∫ pior)
- **Portability**: 94.90% (ü•á melhor!)
- **Distribution**: 2 Excellent + 3 Good + 1 Acceptable (ZERO poor!)
- **Implica√ß√£o**: Mais previs√≠vel que KSWIN, apesar de menor ceiling
- **Recomenda√ß√£o**: Prefer√™ncia quando re-tuning imposs√≠vel

### Insight 4: Parameter Instability
- **Page-Hinkley**: CV=73.7% (muito inst√°vel)
- **HDDM_W**: CV=73.2% (idem)
- **ADWIN**: CV=9.5% (excelente estabilidade)
- **Implica√ß√£o**: Alguns detectores variam muito entre datasets
- **Recomenda√ß√£o**: Avoid production sem extended testing

---

## üîó Refer√™ncia R√°pida

| Pergunta | Resposta | Documento |
|----------|----------|-----------|
| Qual √© o melhor desempenho te√≥rico? | FLOSS (F3=0.4285) | Option 1 |
| Qual posso usar hoje sem tuning? | ADWIN (94.90% retention) | Option 2 |
| Qual √© o melhor equil√≠brio? | KSWIN (0.9690 score) | Option 3 |
| Para dados heterog√™neos? | ADWIN + KSWIN ensemble | Op√ß√£o 2+3 |
| Quantos datasets testaram? | 3 (afib, malign, vtachy) | Todas |
| Qual √© a m√©trica principal? | F3-weighted (recall‚â•4s) | Evaluation metrics |

---

## üìù Estrutura de Ficheiros (Completa)

```
results/cross_dataset_analysis/
‚îú‚îÄ‚îÄ README.md (este ficheiro)
‚îÇ
‚îú‚îÄ‚îÄ OPTION 1 (Ceiling Performance)
‚îÇ   ‚îú‚îÄ‚îÄ cross_dataset_generalization_option1.csv
‚îÇ   ‚îî‚îÄ‚îÄ cross_dataset_generalization_option1.md
‚îÇ
‚îú‚îÄ‚îÄ OPTION 2 (Parameter Portability)
‚îÇ   ‚îú‚îÄ‚îÄ parameter_portability_option2.csv
‚îÇ   ‚îî‚îÄ‚îÄ parameter_portability_option2.md
‚îÇ
‚îú‚îÄ‚îÄ OPTION 3 (Unified Robustness)
‚îÇ   ‚îú‚îÄ‚îÄ unified_robustness_option3.csv
‚îÇ   ‚îî‚îÄ‚îÄ unified_robustness_option3.md
‚îÇ
‚îú‚îÄ‚îÄ 2-FOLD DATA (Supporting)
‚îÇ   ‚îú‚îÄ‚îÄ cross_dataset_generalization_option1.csv (source for Option 1)
‚îÇ   ‚îî‚îÄ‚îÄ twofold_robustness_*.csv (3 files)
‚îÇ
‚îú‚îÄ‚îÄ LEGACY/ARCHIVE
‚îÇ   ‚îú‚îÄ‚îÄ CROSS_DATASET_ANALYSIS_SUMMARY.md
‚îÇ   ‚îú‚îÄ‚îÄ ANALYSIS_RANKING_DISCREPANCIES.md
‚îÇ   ‚îú‚îÄ‚îÄ AGGREGATION_METHODS_COMPARISON.md
‚îÇ   ‚îî‚îÄ‚îÄ <detector>/ (6 dirs with per-detector analysis)
```

---

## üöÄ Pr√≥ximos Passos (Sugerido)

1. **Validar achados**: Compare com cardiologist/ECG domain expert
2. **Escolher detector**: Baseado no cen√°rio (research vs production)
3. **Implementar pipeline**: Use detector selecionado em produ√ß√£o
4. **Monitor performance**: Track F3 real vs expected
5. **Iterate**: Re-tune se performance divergir significativamente

---

**Gerado**: Session 10, 2025-12-14
**Tools**: `aggregate_twofold_analysis.py`, `test_parameter_portability.py`, `unified_robustness_score.py`
