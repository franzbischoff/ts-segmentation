# Detector Comparisons & Analysis

**Ãšltima AtualizaÃ§Ã£o**: 2025-12-15
**Estrutura Reorganizada**: Fase 1 (Limpeza + DocumentaÃ§Ã£o)

---

## ğŸ“ NavegaÃ§Ã£o RÃ¡pida

### ğŸ¯ **1. ComparaÃ§Ãµes por Dataset** â†’ `by_dataset/`

AnÃ¡lises multi-detector por dataset especÃ­fico. Cada pasta contÃ©m:
- **RelatÃ³rio comparativo** (`comparative_report.md`)
- **Rankings & mÃ©tricas** (`detector_rankings.csv`, `detector_summary.csv`)
- **VisualizaÃ§Ãµes** (`visualizations/` com grÃ¡ficos atualizados):
  - `radar_6detectors.png` - VisÃ£o holistada (6 detectores Ã— 6 mÃ©tricas)
  - `f3_vs_fp_scatter.png` - Trade-off performance vs alarmes
  - `heatmap_metrics_comparison.png` - Sensibilidade de detectores
  - `parameter_tradeoffs.png` - Trade-offs multi-objetivo

**Datasets DisponÃ­veis:**
- [**afib_paroxysmal**](by_dataset/afib_paroxysmal/) - 229 ficheiros, 1,301 eventos
- [**malignantventricular**](by_dataset/malignantventricular/) - 22 ficheiros, 592 eventos
- [**vtachyarrhythmias**](by_dataset/vtachyarrhythmias/) - 34 ficheiros, 97 eventos

---

### ğŸŒ **2. AnÃ¡lises Cross-Dataset** â†’ `cross_dataset/`

AnÃ¡lises robustez e portabilidade de detectores **atravÃ©s de mÃºltiplos datasets**. Incluem as **3 opÃ§Ãµes de avaliaÃ§Ã£o**:

#### **OpÃ§Ã£o 1: Performance Ceiling** ğŸ¯
- Pergunta: "Qual Ã© a melhor performance que cada detector consegue atingir (quando tunado)?
- MÃ©trica: F3-weighted mÃ¡ximo por dataset (macro-average)
- Ficheiro: `option1_ceiling_analysis.png`
- Ranking: FLOSS (0.4285) > KSWIN (0.3176) > Page-Hinkley (0.3132)

#### **OpÃ§Ã£o 2: Parameter Portability** ğŸš€
- Pergunta: "Consigo usar hiperparÃ¢metros de um dataset noutro sem re-tuning?"
- MÃ©trica: Transferability ratio (params origem â†’ alvo)
- Ficheiro: `option2_portability_heatmap.png`
- Ranking: ADWIN (94.90%) > KSWIN (87.84%) > FLOSS (75.85%)

#### **OpÃ§Ã£o 3: Unified Robustness Score** âš–ï¸
- Pergunta: "Qual detector Ã© globalmente robusto (combinando ceiling + portabilidade)?"
- FÃ³rmula: `0.6Ã—(1-ceiling_gap) + 0.4Ã—(1-transfer_variance)`
- Ficheiro: `option3_unified_score_ranking.png`
- Ranking: FLOSS (0.9763) > ADWIN (0.9713) > KSWIN (0.9690)

#### **Production Decision Matrix** ğŸ“
- Ficheiro: `production_decision_matrix.png`
- Matriz de decisÃ£o: Qual detector escolher (por cenÃ¡rio)
  - Novo dataset COM labels? â†’ FLOSS + grid search
  - Novo dataset SEM labels? â†’ ADWIN (95% portabilidade)
  - Balanced? â†’ KSWIN (sweet spot)

---

## ğŸ“Š ComparaÃ§Ã£o RÃ¡pida: 3 OpÃ§Ãµes

| Perspetiva | Foco | Top Detector | Score | Use Case |
|-----------|------|---|---|---|
| **OpÃ§Ã£o 1** | ğŸ¯ MÃ¡xima performance | FLOSS | F3=0.4285 | Research, max performance |
| **OpÃ§Ã£o 2** | ğŸš€ Portabilidade | ADWIN | 94.90% | Production ready, sem labels |
| **OpÃ§Ã£o 3** | âš–ï¸ Robustez unificada | FLOSS | 0.9763 | Escolha holÃ­stica |

---

## ğŸ” Como Usar Esta Estrutura

### CenÃ¡rio 1: "Qual detector Ã© melhor para dataset X?"
1. Ir a `by_dataset/<dataset>/`
2. Ler `comparative_report.md`
3. Ver grÃ¡ficos em `visualizations/` (especialmente `heatmap_metrics_comparison.png`)

### CenÃ¡rio 2: "Qual detector escolho para produÃ§Ã£o?"
1. Ir a `cross_dataset/`
2. Ler `production_decision_matrix.png`
3. Se tiver labels â†’ usar OpÃ§Ã£o 1 (FLOSS)
4. Se SEM labels â†’ usar OpÃ§Ã£o 2 (ADWIN)
5. Se quiser balanced â†’ usar OpÃ§Ã£o 3 (KSWIN)

### CenÃ¡rio 3: "Como transferir hiperparÃ¢metros entre datasets?"
1. Ir a `cross_dataset/`
2. Ver `option2_portability_heatmap.png`
3. ADWIN: 95% chance de sucesso (melhor choice)
4. FLOSS: 76% chance (precisa validaÃ§Ã£o)

---

## ğŸ“ Estrutura de Ficheiros

```
comparisons/
â”œâ”€â”€ README.md (este ficheiro)
â”‚
â”œâ”€â”€ by_dataset/
â”‚   â”œâ”€â”€ afib_paroxysmal/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”œâ”€â”€ comparative_report.md
â”‚   â”‚   â”œâ”€â”€ detector_rankings.csv
â”‚   â”‚   â”œâ”€â”€ detector_summary.csv
â”‚   â”‚   â”œâ”€â”€ constraint_tradeoffs.csv
â”‚   â”‚   â”œâ”€â”€ robustness.csv
â”‚   â”‚   â””â”€â”€ visualizations/
â”‚   â”‚       â”œâ”€â”€ radar_6detectors.png
â”‚   â”‚       â”œâ”€â”€ f3_vs_fp_scatter.png
â”‚   â”‚       â”œâ”€â”€ heatmap_metrics_comparison.png
â”‚   â”‚       â””â”€â”€ parameter_tradeoffs.png
â”‚   â”œâ”€â”€ malignantventricular/
â”‚   â”‚   â””â”€â”€ (mesma estrutura)
â”‚   â””â”€â”€ vtachyarrhythmias/
â”‚       â””â”€â”€ (mesma estrutura)
â”‚
â”œâ”€â”€ cross_dataset/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ option123_summary.png (visÃ£o conjunta de 3 opÃ§Ãµes)
â”‚   â”œâ”€â”€ option1_ceiling_analysis.png
â”‚   â”œâ”€â”€ option2_portability_heatmap.png
â”‚   â”œâ”€â”€ option3_unified_score_ranking.png
â”‚   â””â”€â”€ production_decision_matrix.png
â”‚
â””â”€â”€ legacy/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ floss_vs_kswin.md (relatÃ³rio antigo, v1)
    â””â”€â”€ floss_vs_kswin_*.png (visualizaÃ§Ãµes antigas)
```

---

## ğŸ”„ Roadmap de VisualizaÃ§Ãµes

### Fase 1 âœ… (ConcluÃ­da - 2025-12-15)
- [x] Reorganizar estrutura de pastas
- [x] Criar layout hierÃ¡rquico (by_dataset + cross_dataset)
- [x] Preservar ficheiros antigos em legacy/
- [x] DocumentaÃ§Ã£o de navegaÃ§Ã£o

### Fase 2 ğŸ”œ (PrÃ³xima)
- [ ] Gerar visualizaÃ§Ãµes `by_dataset/*/visualizations/`
  - [ ] Script: `src/visualize_comparison_by_dataset.py`
  - [ ] Gera: radar, scatter, heatmap, tradeoffs (6 detectores)

- [ ] Gerar visualizaÃ§Ãµes `cross_dataset/`
  - [ ] Script: `src/visualize_cross_dataset_summary.py`
  - [ ] Gera: ceiling ranking, portability matrix, unified score, decision matrix

- [ ] Criar `generate_comparison_reports.py` (wrapper)
  - [ ] Chama ambos scripts
  - [ ] Organiza saÃ­das automaticamente
  - [ ] Atualiza READMEs

### Fase 3 ğŸ”œ (ValidaÃ§Ã£o)
- [ ] Executar scripts e validar saÃ­das
- [ ] Verificar dimensÃµes, cores, legibilidade dos grÃ¡ficos
- [ ] Cleanup e versionamento Git

---

## ğŸ› ï¸ GeraÃ§Ã£o de RelatÃ³rios

Para atualizar todas as comparaÃ§Ãµes de uma vez:

```bash
# Estrutura jÃ¡ existe; aguardando scripts Python da Fase 2
python -m src.generate_comparison_reports \
    --datasets afib_paroxysmal malignantventricular vtachyarrhythmias \
    --output-base results/comparisons
```

Para comparaÃ§Ã£o por dataset especÃ­fico:

```bash
# Gera comparaÃ§Ãµes + visualizaÃ§Ãµes para um dataset
python -m src.visualize_comparison_by_dataset \
    --dataset afib_paroxysmal \
    --output-dir results/comparisons/by_dataset/afib_paroxysmal/visualizations
```

Para anÃ¡lise cross-dataset:

```bash
# Gera anÃ¡lises de robustez (opÃ§Ãµes 1, 2, 3) + decision matrix
python -m src.visualize_cross_dataset_summary \
    --output-dir results/comparisons/cross_dataset
```

---

## ğŸ“š ReferÃªncias Relacionadas

- **AnÃ¡lises Detalhadas por Detector**: [`results/cross_dataset_analysis/`](../cross_dataset_analysis/)
- **Resultados por Dataset**: [`results/<detector>/`](../)
- **DocumentaÃ§Ã£o de MÃ©tricas**: [`docs/evaluation_metrics_v1.md`](../../docs/evaluation_metrics_v1.md)
- **Guia de VisualizaÃ§Ãµes**: [`docs/visualizations_guide.md`](../../docs/visualizations_guide.md)

---

## ğŸ“Œ Notas Importantes

1. **Ficheiros PNG em `by_dataset/*/visualizations/` ainda estÃ£o a ser gerados** (Fase 2)
   - Por enquanto, usar relatÃ³rios `.md` e CSVs para anÃ¡lise

2. **Legacy folder** contÃ©m comparaÃ§Ãµes antigas (FLOSS vs KSWIN, v1)
   - Preservadas para histÃ³rico; nÃ£o sÃ£o atualizadas

3. **OpÃ§Ãµes 1, 2, 3** sÃ£o complementares, nÃ£o excludentes
   - OpÃ§Ã£o 1: foco em performance mÃ¡xima
   - OpÃ§Ã£o 2: foco em portabilidade
   - OpÃ§Ã£o 3: foco em robustez global
   - Juntas: perspetiva holÃ­stica

4. **READMEs por dataset** serÃ£o criados na Fase 1.3 com:
   - Resumo executivo (top detector por mÃ©trica)
   - RecomendaÃ§Ãµes de uso
   - Links para visualizaÃ§Ãµes
   - Detalhes de trade-offs

---

**PrÃ³ximo Passo**: Fase 1.3 - Criar READMEs estruturais para `by_dataset/` e `cross_dataset/`
