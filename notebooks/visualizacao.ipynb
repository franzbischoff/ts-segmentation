{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa73d5f9",
   "metadata": {},
   "source": [
    "# Notebook de Logging, Grid Search e Visualização de Resultados\n",
    "\n",
    "Este notebook consolida: logging estruturado em JSON, grid search e visualizações de métricas / hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c562b4",
   "metadata": {},
   "source": [
    "## 1. Exportar Dependências para requirements.txt\n",
    "\n",
    "Opções:\n",
    "1. Congelar tudo (`pip freeze`) – menos controlado.\n",
    "2. Selecionar apenas libs nucleares que importamos.\n",
    "\n",
    "Abaixo: coleta módulos carregados relevantes + interseção com `pip freeze`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f7d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "from importlib import util\n",
    "\n",
    "# Fix para subprocess e parsing de requirements\n",
    "\n",
    "CORE = {\"numpy\", \"pandas\", \"scikit-learn\", \"scikit-multiflow\", \"matplotlib\", \"seaborn\"}\n",
    "\n",
    "try:\n",
    "    raw_freeze = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'], text=True).strip().split('\\n')\n",
    "    parsed = {}\n",
    "    for line in raw_freeze:\n",
    "        if '==' in line:\n",
    "            pkg, ver = line.split('==', 1)\n",
    "            parsed[pkg.lower()] = line\n",
    "\n",
    "    selected = [parsed[p.lower()] for p in CORE if p.lower() in parsed]\n",
    "    print('Selecionados:')\n",
    "    print('\\n'.join(sorted(selected)))\n",
    "\n",
    "    # Create parent directory if it doesn't exist\n",
    "    parent_dir = pathlib.Path('../')\n",
    "    parent_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    requirements_path = parent_dir / 'requirements_minimal.txt'\n",
    "    with open(requirements_path, 'w') as f:\n",
    "        f.write('\\n'.join(sorted(selected)) + '\\n')\n",
    "    print(f'Arquivo gravado em {requirements_path}')\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f'Erro ao executar pip freeze: {e}')\n",
    "except Exception as e:\n",
    "    print(f'Erro: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc515794",
   "metadata": {},
   "source": [
    "## 2. Configurar Logging JSON Estruturado\n",
    "\n",
    "Criaremos um logger com formatter que serializa dicts para JSON, incluindo campos extra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6a9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "class JsonFormatter(logging.Formatter):\n",
    "    def format(self, record: logging.LogRecord):\n",
    "        base = {\n",
    "            'ts': datetime.now(timezone.utc).isoformat(),\n",
    "            'level': record.levelname,\n",
    "            'logger': record.name,\n",
    "            'msg': record.getMessage(),\n",
    "            'correlation_id': getattr(record, 'correlation_id', None)\n",
    "        }\n",
    "        # Merge extras (safe)\n",
    "        excluded_attrs = {\n",
    "            'msg', 'args', 'exc_info', 'exc_text', 'stack_info', 'levelno',\n",
    "            'levelname', 'name', 'thread', 'threadName', 'processName',\n",
    "            'process', 'created', 'msecs', 'relativeCreated', 'pathname',\n",
    "            'filename', 'module', 'lineno', 'funcName'\n",
    "        }\n",
    "\n",
    "        for k, v in record.__dict__.items():\n",
    "            if k.startswith('_') or k in base or k in excluded_attrs:\n",
    "                continue\n",
    "            try:\n",
    "                json.dumps(v)  # test serializável\n",
    "                base[k] = v\n",
    "            except Exception:\n",
    "                base[k] = repr(v)\n",
    "\n",
    "        if record.exc_info:\n",
    "            base['exception'] = self.formatException(record.exc_info)\n",
    "        return json.dumps(base, ensure_ascii=False)\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir = pathlib.Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "LOG_CFG = {\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': False,\n",
    "    'formatters': {\n",
    "        'json': {'()': JsonFormatter}\n",
    "    },\n",
    "    'handlers': {\n",
    "        'console': {\n",
    "            'class': 'logging.StreamHandler',\n",
    "            'formatter': 'json',\n",
    "            'stream': 'ext://sys.stdout'\n",
    "        },\n",
    "        'file': {\n",
    "            'class': 'logging.FileHandler',\n",
    "            'formatter': 'json',\n",
    "            'filename': str(results_dir / 'notebook_log.jsonl'),\n",
    "            'mode': 'a',\n",
    "            'encoding': 'utf-8'\n",
    "        }\n",
    "    },\n",
    "    'root': {\n",
    "        'handlers': ['console', 'file'],\n",
    "        'level': 'INFO'\n",
    "    }\n",
    "}\n",
    "\n",
    "logging.config.dictConfig(LOG_CFG)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info('Logger inicializado', extra={'phase': 'init'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945661b5",
   "metadata": {},
   "source": [
    "## 3. Adicionar Correlation ID e Context Manager\n",
    "Usamos `contextvars` para propagar um correlation_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c3b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextvars\n",
    "import uuid\n",
    "import functools\n",
    "import contextlib\n",
    "\n",
    "correlation_var = contextvars.ContextVar('correlation_id', default=None)\n",
    "\n",
    "class CorrelationFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        cid = correlation_var.get()\n",
    "        if cid:\n",
    "            record.correlation_id = cid\n",
    "        return True\n",
    "\n",
    "# Add correlation filter to all existing handlers\n",
    "for handler in logging.getLogger().handlers:\n",
    "    handler.addFilter(CorrelationFilter())\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def correlation_context(cid: str = None):\n",
    "    \"\"\"Context manager for correlation ID\"\"\"\n",
    "    if cid is None:\n",
    "        cid = str(uuid.uuid4())\n",
    "    token = correlation_var.set(cid)\n",
    "    try:\n",
    "        yield correlation_var.get()\n",
    "    finally:\n",
    "        correlation_var.reset(token)\n",
    "\n",
    "# Demo usage\n",
    "with correlation_context() as cid:\n",
    "    logger.info('Exemplo de log com correlation id', extra={'cid_demo': True, 'correlation_id_demo': cid})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667369f",
   "metadata": {},
   "source": [
    "## 4. Rotação e Compressão de Logs\n",
    "Pós-processamos arquivos rotacionados compactando-os em `.gz`. Exemplo simplificado abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c02d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, shutil, glob\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "\n",
    "class GzipTimedRotatingFileHandler(TimedRotatingFileHandler):\n",
    "    def rotate(self, source, dest):\n",
    "        super().rotate(source, dest)\n",
    "        if os.path.exists(dest):\n",
    "            with open(dest,'rb') as f_in, gzip.open(dest + '.gz','wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "            os.remove(dest)\n",
    "\n",
    "# (Demonstração, não substitui handler configurado previamente)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837f49d8",
   "metadata": {},
   "source": [
    "## 5. Função de Métricas e Fórmulas\n",
    "\n",
    "$Precision=\\frac{TP}{TP+FP}$  \\\n",
    "$Recall=\\frac{TP}{TP+FN}$  \\\n",
    "$F1=\\frac{2PR}{P+R}$\n",
    "\n",
    "Implementação simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def compute_metrics(tp:int, fp:int, fn:int, tn:int|None=None) -> Dict[str,float]:\n",
    "    precision = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
    "    recall = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
    "    f1 = (2*precision*recall/(precision+recall)) if (precision+recall)>0 else 0.0\n",
    "    acc = None\n",
    "    if tn is not None:\n",
    "        acc = (tp + tn) / (tp + tn + fp + fn) if (tp+tn+fp+fn)>0 else 0.0\n",
    "    return {'precision':precision,'recall':recall,'f1':f1,'accuracy':acc}\n",
    "\n",
    "print(compute_metrics(10,5,2, tn=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd03660a",
   "metadata": {},
   "source": [
    "## 6. Definir Espaço de Busca (Grid Search)\n",
    "Exemplo genérico de hiperparâmetros (ilustrativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a612f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, pandas as pd\n",
    "param_grid = {\n",
    "  'lr':[1e-4,1e-3],\n",
    "  'batch_size':[16,32],\n",
    "  'hidden':[64,128]\n",
    "}\n",
    "rows=[]\n",
    "for combo in itertools.product(*param_grid.values()):\n",
    "    rows.append(dict(zip(param_grid.keys(), combo)))\n",
    "param_df = pd.DataFrame(rows)\n",
    "param_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eed31d",
   "metadata": {},
   "source": [
    "## 7. Função de Treinamento/Avaliação do Modelo (Exemplo)\n",
    "Usamos LogisticRegression do scikit-learn para demonstrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916f9702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np, time\n",
    "\n",
    "X, y = make_classification(n_samples=1500, n_features=20, n_informative=8, random_state=42)\n",
    "Xtr, Xte, ytr, yte = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "\n",
    "def train_eval(params: dict):\n",
    "    start = time.time()\n",
    "    model = LogisticRegression(max_iter=500, C=1/params['lr'])\n",
    "    model.fit(Xtr, ytr)\n",
    "    pred = model.predict(Xte)\n",
    "    p = precision_score(yte, pred)\n",
    "    r = recall_score(yte, pred)\n",
    "    f1 = f1_score(yte, pred)\n",
    "    return {**params, 'precision':p,'recall':r,'f1':f1,'duration': time.time()-start}\n",
    "\n",
    "print(train_eval({'lr':1e-3,'batch_size':32,'hidden':64}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2672ac64",
   "metadata": {},
   "source": [
    "## 8. Executor de Grid Search Paralelo (joblib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a85cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "results = Parallel(n_jobs=-1)(delayed(train_eval)(row) for row in rows)\n",
    "len(results), results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d577a483",
   "metadata": {},
   "source": [
    "## 9. Persistir e Carregar Resultados do Grid Search\n",
    "Salvamos JSONL + Parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99818616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "import os\n",
    "\n",
    "# Verify pyarrow is available\n",
    "try:\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    PARQUET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: pyarrow not available, skipping Parquet export\")\n",
    "    PARQUET_AVAILABLE = False\n",
    "\n",
    "res_df = pd.DataFrame(results)\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save to JSONL format\n",
    "with open('../results/grid_results.jsonl','w') as f:\n",
    "    for rec in res_df.to_dict(orient='records'):\n",
    "        f.write(json.dumps(rec) + '\\n')\n",
    "\n",
    "# Save to Parquet format if available\n",
    "if PARQUET_AVAILABLE:\n",
    "    table = pa.Table.from_pandas(res_df)\n",
    "    pq.write_table(table, '../results/grid_results.parquet')\n",
    "    print('Persistido em JSONL e Parquet.')\n",
    "else:\n",
    "    print('Persistido em JSONL apenas.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876a5b3",
   "metadata": {},
   "source": [
    "## 10. Gerar Script Externo de Grid Search (arquivo .py)\n",
    "Criamos um script modular reutilizável."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040c44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_code = '''#!/usr/bin/env python3\n",
    "import argparse\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "import os\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def train_eval(params, data):\n",
    "    \"\"\"Train and evaluate model with given parameters\"\"\"\n",
    "    Xtr, Xte, ytr, yte = data\n",
    "    start = time.time()\n",
    "    model = LogisticRegression(max_iter=500, C=1/params['lr'])\n",
    "    model.fit(Xtr, ytr)\n",
    "    pred = model.predict(Xte)\n",
    "\n",
    "    precision = precision_score(yte, pred)\n",
    "    recall = recall_score(yte, pred)\n",
    "    f1 = f1_score(yte, pred)\n",
    "    duration = time.time() - start\n",
    "\n",
    "    return {\n",
    "        **params,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'duration': duration\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Grid Search for ML model')\n",
    "    parser.add_argument('--out', default='results/gs_external.jsonl', help='Output file path')\n",
    "    parser.add_argument('--n-jobs', type=int, default=-1, help='Number of parallel jobs')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Create results directory\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Generate synthetic data\n",
    "    X, y = make_classification(\n",
    "        n_samples=1200,\n",
    "        n_features=20,\n",
    "        n_informative=8,\n",
    "        random_state=42\n",
    "    )\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Define grid\n",
    "    grid = {\n",
    "        'lr': [1e-4, 1e-3],\n",
    "        'batch_size': [16, 32],\n",
    "        'hidden': [64, 128]\n",
    "    }\n",
    "\n",
    "    # Generate parameter combinations\n",
    "    combos = [\n",
    "        dict(zip(grid.keys(), vals))\n",
    "        for vals in itertools.product(*grid.values())\n",
    "    ]\n",
    "\n",
    "    print(f'Running grid search with {len(combos)} combinations...')\n",
    "\n",
    "    # Run parallel grid search\n",
    "    results = Parallel(n_jobs=args.n_jobs)(\n",
    "        delayed(train_eval)(params, (Xtr, Xte, ytr, yte))\n",
    "        for params in combos\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    with open(args.out, 'w') as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + '\\\\n')\n",
    "\n",
    "    print(f'Results saved to {args.out}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Write the script to file\n",
    "script_path = '../grid_search_external.py'\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(script_code)\n",
    "print(f'Script escrito em {script_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0ac5ad",
   "metadata": {},
   "source": [
    "## 11. Notebook de Visualização: Carregar Resultados Salvos\n",
    "Carrega JSONL ou Parquet gerados anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8419da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json\n",
    "from pathlib import Path\n",
    "jsonl_path = Path('../results/grid_results.jsonl')\n",
    "rows=[]\n",
    "if jsonl_path.exists():\n",
    "    with open(jsonl_path) as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "vis_df = pd.DataFrame(rows)\n",
    "vis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837ee4a",
   "metadata": {},
   "source": [
    "## 12. Visualizações: Heatmap de Hiperparâmetros\n",
    "Pivot de f1 por lr x batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059dff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if not vis_df.empty and 'lr' in vis_df.columns and 'batch_size' in vis_df.columns and 'f1' in vis_df.columns:\n",
    "    try:\n",
    "        pivot = vis_df.pivot_table(index='lr', columns='batch_size', values='f1', aggfunc='mean')\n",
    "        plt.figure(figsize=(5, 4))\n",
    "        sns.heatmap(pivot, annot=True, cmap='viridis', fmt='.3f')\n",
    "        plt.title('F1 por lr x batch_size')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao criar heatmap: {e}')\n",
    "else:\n",
    "    print('DataFrame vazio ou colunas ausentes para heatmap')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9fe6c",
   "metadata": {},
   "source": [
    "## 13. Visualizações: Distribuições e Pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not vis_df.empty and 'f1' in vis_df.columns and 'duration' in vis_df.columns:\n",
    "    try:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "        # F1 distribution histogram\n",
    "        vis_df['f1'].hist(ax=ax[0], bins=10, alpha=0.7)\n",
    "        ax[0].set_title('Distribuição F1')\n",
    "        ax[0].set_xlabel('F1 Score')\n",
    "        ax[0].set_ylabel('Frequência')\n",
    "\n",
    "        # Pareto plot: duration vs F1\n",
    "        ax[1].scatter(vis_df['duration'], vis_df['f1'], alpha=0.7)\n",
    "        ax[1].set_xlabel('Duração (s)')\n",
    "        ax[1].set_ylabel('F1 Score')\n",
    "        ax[1].set_title('Pareto: Tempo vs F1')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao criar visualizações: {e}')\n",
    "else:\n",
    "    print('Sem dados ou colunas ausentes para distribuições')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6532d4c",
   "metadata": {},
   "source": [
    "## 14. Visualizações: Evolução da Métrica\n",
    "Ordena execuções e mostra melhor cumulativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac0e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not vis_df.empty and 'f1' in vis_df.columns:\n",
    "    try:\n",
    "        vis_df_copy = vis_df.copy().reset_index(drop=True)\n",
    "        best_so_far = vis_df_copy['f1'].cummax()\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.plot(vis_df_copy.index, vis_df_copy['f1'], 'o-', label='F1 por execução', alpha=0.7)\n",
    "        plt.plot(vis_df_copy.index, best_so_far, '--', label='Melhor até aqui', linewidth=2)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Número da Execução')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('Evolução do F1 Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f'Erro ao criar gráfico de evolução: {e}')\n",
    "else:\n",
    "    print('Sem dados ou coluna F1 ausente para evolução')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ab2a14",
   "metadata": {},
   "source": [
    "## 15. Dashboard Interativo (Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209de972",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import plotly.express as px\n",
    "    PLOTLY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: plotly not available, skipping 3D visualization\")\n",
    "    PLOTLY_AVAILABLE = False\n",
    "\n",
    "if PLOTLY_AVAILABLE and not vis_df.empty:\n",
    "    required_cols = ['lr', 'hidden', 'f1', 'batch_size']\n",
    "    if all(col in vis_df.columns for col in required_cols):\n",
    "        try:\n",
    "            fig = px.scatter_3d(\n",
    "                vis_df,\n",
    "                x='lr',\n",
    "                y='hidden',\n",
    "                z='f1',\n",
    "                color='batch_size',\n",
    "                size='f1',\n",
    "                title='Exploração 3D dos Hiperparâmetros',\n",
    "                labels={'lr': 'Learning Rate', 'hidden': 'Hidden Units', 'f1': 'F1 Score'}\n",
    "            )\n",
    "            fig.show()\n",
    "        except Exception as e:\n",
    "            print(f'Erro ao criar plot 3D: {e}')\n",
    "    else:\n",
    "        missing_cols = [col for col in required_cols if col not in vis_df.columns]\n",
    "        print(f'Colunas ausentes para plot 3D: {missing_cols}')\n",
    "else:\n",
    "    if not PLOTLY_AVAILABLE:\n",
    "        print('Plotly não disponível')\n",
    "    else:\n",
    "        print('Sem dados para plot interativo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45abf51f",
   "metadata": {},
   "source": [
    "## 16. Integração Logging + Resultados (Enriquecer Registros)\n",
    "Função para logar resumo formatado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_result(record: dict):\n",
    "    \"\"\"Log a result record with proper serialization\"\"\"\n",
    "    try:\n",
    "        # Ensure all values are JSON serializable\n",
    "        serializable_record = {}\n",
    "        for key, value in record.items():\n",
    "            try:\n",
    "                json.dumps(value)\n",
    "                serializable_record[key] = value\n",
    "            except (TypeError, ValueError):\n",
    "                serializable_record[key] = str(value)\n",
    "\n",
    "        logger.info('Grid search result', extra={\n",
    "            'tag': 'result',\n",
    "            'payload': serializable_record\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to log result: {e}', extra={'error_type': 'log_result_error'})\n",
    "\n",
    "# Log best result if available\n",
    "if not vis_df.empty and 'f1' in vis_df.columns:\n",
    "    try:\n",
    "        best_result = vis_df.sort_values('f1', ascending=False).iloc[0]\n",
    "        log_result(best_result.to_dict())\n",
    "        print(f\"Melhor resultado logado: F1 = {best_result['f1']:.4f}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f'Failed to log best result: {e}')\n",
    "        print(f'Erro ao logar melhor resultado: {e}')\n",
    "else:\n",
    "    logger.warning('Sem resultados para log_result', extra={'warning_type': 'no_results'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b26812",
   "metadata": {},
   "source": [
    "## 17. Exportar Relatório Consolidado\n",
    "Gera markdown com top configuração e estatísticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58947cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from statistics import mean, pstdev\n",
    "\n",
    "report_path = '../results/report.md'\n",
    "\n",
    "if not vis_df.empty:\n",
    "    top = vis_df.sort_values('f1', ascending=False).iloc[0]\n",
    "    stats = {\n",
    "        'f1_mean': vis_df['f1'].mean(),\n",
    "        'f1_std': vis_df['f1'].std(),\n",
    "        'f1_min': vis_df['f1'].min(),\n",
    "        'f1_max': vis_df['f1'].max(),\n",
    "        'n': len(vis_df)\n",
    "    }\n",
    "\n",
    "    # Generate markdown report with proper formatting\n",
    "    md_lines = [\n",
    "        '# Relatório Grid Search',\n",
    "        '',\n",
    "        '## Top Configuração',\n",
    "        '```json',\n",
    "        json.dumps(top.to_dict(), indent=2),\n",
    "        '```',\n",
    "        '',\n",
    "        '## Estatísticas',\n",
    "        '```json',\n",
    "        json.dumps(stats, indent=2),\n",
    "        '```'\n",
    "    ]\n",
    "\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write('\\n'.join(md_lines))\n",
    "\n",
    "    logger.info('Relatório gerado', extra={'report_path': report_path, 'top_f1': float(top['f1'])})\n",
    "    print('Gerado', report_path)\n",
    "else:\n",
    "    print('Sem dados para relatório')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
